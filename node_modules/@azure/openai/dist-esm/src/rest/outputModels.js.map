{"version":3,"file":"outputModels.js","sourceRoot":"","sources":["../../../src/rest/outputModels.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/**\n * THIS IS AN AUTO-GENERATED FILE - DO NOT EDIT!\n *\n * Any changes you make here may be lost.\n *\n * If you need to make changes, please do so in the original source file, \\{project-root\\}/sources/custom\n */\n\nimport { ErrorModel } from \"@azure-rest/core-client\";\n\n/** A specific deployment */\nexport interface DeploymentOutput {\n  /** Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. */\n  readonly deploymentId: string;\n}\n\n/** Result information for an operation that transcribed spoken audio into written text. */\nexport interface AudioTranscriptionOutput {\n  /** The transcribed text for the provided audio data. */\n  text: string;\n  /**\n   * The label that describes which operation type generated the accompanying response data.\n   *\n   * Possible values: transcribe, translate\n   */\n  task?: string;\n  /**\n   * The spoken language that was detected in the transcribed audio data.\n   * This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.\n   */\n  language?: string;\n  /** The total duration of the audio processed to produce accompanying transcription information. */\n  duration?: number;\n  /** A collection of information about the timing, probabilities, and other detail of each processed audio segment. */\n  segments?: Array<AudioTranscriptionSegmentOutput>;\n}\n\n/**\n * Extended information about a single segment of transcribed audio data.\n * Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur between words but not\n * necessarily sentences.\n */\nexport interface AudioTranscriptionSegmentOutput {\n  /** The 0-based index of this segment within a transcription. */\n  id: number;\n  /** The time at which this segment started relative to the beginning of the transcribed audio. */\n  start: number;\n  /** The time at which this segment ended relative to the beginning of the transcribed audio. */\n  end: number;\n  /** The transcribed text that was part of this audio segment. */\n  text: string;\n  /** The temperature score associated with this audio segment. */\n  temperature: number;\n  /** The average log probability associated with this audio segment. */\n  avg_logprob: number;\n  /** The compression ratio of this audio segment. */\n  compression_ratio: number;\n  /** The probability of no speech detection within this audio segment. */\n  no_speech_prob: number;\n  /** The token IDs matching the transcribed text in this audio segment. */\n  tokens: number[];\n  /**\n   * The seek position associated with the processing of this audio segment.\n   * Seek positions are expressed as hundredths of seconds.\n   * The model may process several segments from a single seek position, so while the seek position will never represent\n   * a later time than the segment's start, the segment's start may represent a significantly later time than the\n   * segment's associated seek position.\n   */\n  seek: number;\n}\n\n/** Result information for an operation that translated spoken audio into written text. */\nexport interface AudioTranslationOutput {\n  /** The translated text for the provided audio data. */\n  text: string;\n  /**\n   * The label that describes which operation type generated the accompanying response data.\n   *\n   * Possible values: transcribe, translate\n   */\n  task?: string;\n  /**\n   * The spoken language that was detected in the translated audio data.\n   * This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.\n   */\n  language?: string;\n  /** The total duration of the audio processed to produce accompanying translation information. */\n  duration?: number;\n  /** A collection of information about the timing, probabilities, and other detail of each processed audio segment. */\n  segments?: Array<AudioTranslationSegmentOutput>;\n}\n\n/**\n * Extended information about a single segment of translated audio data.\n * Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur between words but not\n * necessarily sentences.\n */\nexport interface AudioTranslationSegmentOutput {\n  /** The 0-based index of this segment within a translation. */\n  id: number;\n  /** The time at which this segment started relative to the beginning of the translated audio. */\n  start: number;\n  /** The time at which this segment ended relative to the beginning of the translated audio. */\n  end: number;\n  /** The translated text that was part of this audio segment. */\n  text: string;\n  /** The temperature score associated with this audio segment. */\n  temperature: number;\n  /** The average log probability associated with this audio segment. */\n  avg_logprob: number;\n  /** The compression ratio of this audio segment. */\n  compression_ratio: number;\n  /** The probability of no speech detection within this audio segment. */\n  no_speech_prob: number;\n  /** The token IDs matching the translated text in this audio segment. */\n  tokens: number[];\n  /**\n   * The seek position associated with the processing of this audio segment.\n   * Seek positions are expressed as hundredths of seconds.\n   * The model may process several segments from a single seek position, so while the seek position will never represent\n   * a later time than the segment's start, the segment's start may represent a significantly later time than the\n   * segment's associated seek position.\n   */\n  seek: number;\n}\n\n/**\n * Representation of the response data from a completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface CompletionsOutput {\n  /** A unique identifier associated with this completions response. */\n  id: string;\n  /**\n   * The first timestamp associated with generation activity for this completions response,\n   * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.\n   */\n  created: number;\n  /**\n   * Content filtering results for zero or more prompts in the request. In a streaming request,\n   * results for different prompts may arrive at different times or in different orders.\n   */\n  prompt_filter_results?: Array<ContentFilterResultsForPromptOutput>;\n  /**\n   * The collection of completions choices associated with this completions response.\n   * Generally, `n` choices are generated per provided prompt with a default value of 1.\n   * Token limits and other settings may limit the number of choices generated.\n   */\n  choices: Array<ChoiceOutput>;\n  /** Usage information for tokens processed and generated as part of this completions operation. */\n  usage: CompletionsUsageOutput;\n}\n\n/** Content filtering results for a single prompt in the request. */\nexport interface ContentFilterResultsForPromptOutput {\n  /** The index of this prompt in the set of prompt results */\n  prompt_index: number;\n  /** Content filtering results for this prompt */\n  content_filter_results: ContentFilterResultDetailsForPromptOutput;\n}\n\n/** Information about content filtering evaluated against input data to Azure OpenAI. */\nexport interface ContentFilterResultDetailsForPromptOutput {\n  /**\n   * Describes language related to anatomical organs and genitals, romantic relationships,\n   *  acts portrayed in erotic or affectionate terms, physical sexual acts, including\n   *  those portrayed as an assault or a forced sexual violent act against one’s will,\n   *  prostitution, pornography, and abuse.\n   */\n  sexual?: ContentFilterResultOutput;\n  /**\n   * Describes language related to physical actions intended to hurt, injure, damage, or\n   * kill someone or something; describes weapons, etc.\n   */\n  violence?: ContentFilterResultOutput;\n  /**\n   * Describes language attacks or uses that include pejorative or discriminatory language\n   * with reference to a person or identity group on the basis of certain differentiating\n   * attributes of these groups including but not limited to race, ethnicity, nationality,\n   * gender identity and expression, sexual orientation, religion, immigration status, ability\n   * status, personal appearance, and body size.\n   */\n  hate?: ContentFilterResultOutput;\n  /**\n   * Describes language related to physical actions intended to purposely hurt, injure,\n   * or damage one’s body, or kill oneself.\n   */\n  self_harm?: ContentFilterResultOutput;\n  /** Describes whether profanity was detected. */\n  profanity?: ContentFilterDetectionResultOutput;\n  /** Describes detection results against configured custom blocklists. */\n  custom_blocklists?: Array<ContentFilterBlocklistIdResultOutput>;\n  /**\n   * Describes an error returned if the content filtering system is\n   * down or otherwise unable to complete the operation in time.\n   */\n  error?: ErrorModel;\n  /** Whether a jailbreak attempt was detected in the prompt. */\n  jailbreak?: ContentFilterDetectionResultOutput;\n}\n\n/** Information about filtered content severity level and if it has been filtered or not. */\nexport interface ContentFilterResultOutput {\n  /**\n   * Ratings for the intensity and risk level of filtered content.\n   *\n   * Possible values: safe, low, medium, high\n   */\n  severity: string;\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n}\n\n/** Represents the outcome of a detection operation performed by content filtering. */\nexport interface ContentFilterDetectionResultOutput {\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n  /** A value indicating whether detection occurred, irrespective of severity or whether the content was filtered. */\n  detected: boolean;\n}\n\n/** Represents the outcome of an evaluation against a custom blocklist as performed by content filtering. */\nexport interface ContentFilterBlocklistIdResultOutput {\n  /** The ID of the custom blocklist evaluated. */\n  id: string;\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n}\n\n/**\n * The representation of a single prompt completion as part of an overall completions request.\n * Generally, `n` choices are generated per provided prompt with a default value of 1.\n * Token limits and other settings may limit the number of choices generated.\n */\nexport interface ChoiceOutput {\n  /** The generated text for a given completions prompt. */\n  text: string;\n  /** The ordered index associated with this completions choice. */\n  index: number;\n  /**\n   * Information about the content filtering category (hate, sexual, violence, self_harm), if it\n   * has been detected, as well as the severity level (very_low, low, medium, high-scale that\n   * determines the intensity and risk level of harmful content) and if it has been filtered or not.\n   */\n  content_filter_results?: ContentFilterResultsForChoiceOutput;\n  /** The log probabilities model for tokens associated with this completions choice. */\n  logprobs: CompletionsLogProbabilityModelOutput | null;\n  /** Reason for finishing */\n  finish_reason: string | null;\n}\n\n/** Information about content filtering evaluated against generated model output. */\nexport interface ContentFilterResultsForChoiceOutput {\n  /**\n   * Describes language related to anatomical organs and genitals, romantic relationships,\n   *  acts portrayed in erotic or affectionate terms, physical sexual acts, including\n   *  those portrayed as an assault or a forced sexual violent act against one’s will,\n   *  prostitution, pornography, and abuse.\n   */\n  sexual?: ContentFilterResultOutput;\n  /**\n   * Describes language related to physical actions intended to hurt, injure, damage, or\n   * kill someone or something; describes weapons, etc.\n   */\n  violence?: ContentFilterResultOutput;\n  /**\n   * Describes language attacks or uses that include pejorative or discriminatory language\n   * with reference to a person or identity group on the basis of certain differentiating\n   * attributes of these groups including but not limited to race, ethnicity, nationality,\n   * gender identity and expression, sexual orientation, religion, immigration status, ability\n   * status, personal appearance, and body size.\n   */\n  hate?: ContentFilterResultOutput;\n  /**\n   * Describes language related to physical actions intended to purposely hurt, injure,\n   * or damage one’s body, or kill oneself.\n   */\n  self_harm?: ContentFilterResultOutput;\n  /** Describes whether profanity was detected. */\n  profanity?: ContentFilterDetectionResultOutput;\n  /** Describes detection results against configured custom blocklists. */\n  custom_blocklists?: Array<ContentFilterBlocklistIdResultOutput>;\n  /**\n   * Describes an error returned if the content filtering system is\n   * down or otherwise unable to complete the operation in time.\n   */\n  error?: ErrorModel;\n  /** Information about detection of protected text material. */\n  protected_material_text?: ContentFilterDetectionResultOutput;\n  /** Information about detection of protected code material. */\n  protected_material_code?: ContentFilterCitedDetectionResultOutput;\n}\n\n/** Represents the outcome of a detection operation against protected resources as performed by content filtering. */\nexport interface ContentFilterCitedDetectionResultOutput {\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n  /** A value indicating whether detection occurred, irrespective of severity or whether the content was filtered. */\n  detected: boolean;\n  /** The internet location associated with the detection. */\n  URL?: string;\n  /** The license description associated with the detection. */\n  license: string;\n}\n\n/** Representation of a log probabilities model for a completions generation. */\nexport interface CompletionsLogProbabilityModelOutput {\n  /** The textual forms of tokens evaluated in this probability model. */\n  tokens: string[];\n  /** A collection of log probability values for the tokens in this completions data. */\n  token_logprobs: (number | null)[];\n  /** A mapping of tokens to maximum log probability values in this completions data. */\n  top_logprobs: Record<string, number | null>[];\n  /** The text offsets associated with tokens in this completions data. */\n  text_offset: number[];\n}\n\n/**\n * Representation of the token counts processed for a completions request.\n * Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and\n * other consumers.\n */\nexport interface CompletionsUsageOutput {\n  /** The number of tokens generated across all completions emissions. */\n  completion_tokens: number;\n  /** The number of tokens in the provided prompts for the completions request. */\n  prompt_tokens: number;\n  /** The total number of tokens processed for the completions request and response. */\n  total_tokens: number;\n}\n\n/**\n * An abstract representation of a tool call that must be resolved in a subsequent request to perform the requested\n * chat completion.\n */\nexport interface ChatCompletionsToolCallOutputParent {\n  /** The ID of the tool call. */\n  id: string;\n  type: string;\n}\n\n/**\n * A tool call to a function tool, issued by the model in evaluation of a configured function tool, that represents\n * a function invocation needed for a subsequent chat completions request to resolve.\n */\nexport interface ChatCompletionsFunctionToolCallOutput extends ChatCompletionsToolCallOutputParent {\n  /** The type of tool call, in this case always 'function'. */\n  type: \"function\";\n  /** The details of the function invocation requested by the tool call. */\n  function: FunctionCallOutput;\n}\n\n/** The name and arguments of a function that should be called, as generated by the model. */\nexport interface FunctionCallOutput {\n  /** The name of the function to call. */\n  name: string;\n  /**\n   * The arguments to call the function with, as generated by the model in JSON format.\n   * Note that the model does not always generate valid JSON, and may hallucinate parameters\n   * not defined by your function schema. Validate the arguments in your code before calling\n   * your function.\n   */\n  arguments: string;\n}\n\n/**\n * Representation of the response data from a chat completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface ChatCompletionsOutput {\n  /** A unique identifier associated with this chat completions response. */\n  id: string;\n  /**\n   * The first timestamp associated with generation activity for this completions response,\n   * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.\n   */\n  created: number;\n  /**\n   * The collection of completions choices associated with this completions response.\n   * Generally, `n` choices are generated per provided prompt with a default value of 1.\n   * Token limits and other settings may limit the number of choices generated.\n   */\n  choices: Array<ChatChoiceOutput>;\n  /**\n   * Content filtering results for zero or more prompts in the request. In a streaming request,\n   * results for different prompts may arrive at different times or in different orders.\n   */\n  prompt_filter_results: Array<ContentFilterResultsForPromptOutput>;\n  /**\n   * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that\n   * might impact determinism.\n   */\n  system_fingerprint: string;\n  /** Usage information for tokens processed and generated as part of this completions operation. */\n  usage: CompletionsUsageOutput;\n}\n\n/**\n * The representation of a single prompt completion as part of an overall chat completions request.\n * Generally, `n` choices are generated per provided prompt with a default value of 1.\n * Token limits and other settings may limit the number of choices generated.\n */\nexport interface ChatChoiceOutput {\n  /** The chat message for a given chat completions prompt. */\n  message?: ChatResponseMessageOutput;\n  /** The ordered index associated with this chat completions choice. */\n  index: number;\n  /** The reason that this chat completions choice completed its generated. */\n  finish_reason: string | null;\n  /**\n   * The reason the model stopped generating tokens, together with any applicable details.\n   * This structured representation replaces 'finish_reason' for some models.\n   */\n  finish_details?: ChatFinishDetailsOutput;\n  /** The delta message content for a streaming response. */\n  delta?: ChatResponseMessageOutput;\n  /**\n   * Information about the content filtering category (hate, sexual, violence, self_harm), if it\n   * has been detected, as well as the severity level (very_low, low, medium, high-scale that\n   * determines the intensity and risk level of harmful content) and if it has been filtered or not.\n   */\n  content_filter_results?: ContentFilterResultsForChoiceOutput;\n  /**\n   * Represents the output results of Azure OpenAI enhancements to chat completions, as configured via the matching input\n   * provided in the request. This supplementary information is only available when using Azure OpenAI and only when the\n   * request is configured to use enhancements.\n   */\n  enhancements?: AzureChatEnhancementsOutput;\n}\n\n/** A representation of a chat message as received in a response. */\nexport interface ChatResponseMessageOutput {\n  /**\n   * The chat role associated with the message.\n   *\n   * Possible values: system, assistant, user, function, tool\n   */\n  role: string;\n  /** The content of the message. */\n  content: string | null;\n  /**\n   * The tool calls that must be resolved and have their outputs appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  tool_calls?: Array<ChatCompletionsToolCallOutput>;\n  /**\n   * The function call that must be resolved and have its output appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  function_call?: FunctionCallOutput;\n  /**\n   * If Azure OpenAI chat extensions are configured, this array represents the incremental steps performed by those\n   * extensions while processing the chat completions request.\n   */\n  context?: AzureChatExtensionsMessageContextOutput;\n}\n\n/**\n *   A representation of the additional context information available when Azure OpenAI chat extensions are involved\n *   in the generation of a corresponding chat completions response. This context information is only populated when\n *   using an Azure OpenAI request configured to use a matching extension.\n */\nexport interface AzureChatExtensionsMessageContextOutput {\n  /**\n   *   The contextual message payload associated with the Azure chat extensions used for a chat completions request.\n   *   These messages describe the data source retrievals, plugin invocations, and other intermediate steps taken in the\n   *   course of generating a chat completions response that was augmented by capabilities from Azure OpenAI chat\n   *   extensions.\n   */\n  messages?: Array<ChatResponseMessageOutput>;\n}\n\n/** An abstract representation of structured information about why a chat completions response terminated. */\nexport interface ChatFinishDetailsOutputParent {\n  type: string;\n}\n\n/** A structured representation of a stop reason that signifies natural termination by the model. */\nexport interface StopFinishDetailsOutput extends ChatFinishDetailsOutputParent {\n  /** The object type, which is always 'stop' for this object. */\n  type: \"stop\";\n  /** The token sequence that the model terminated with. */\n  stop: string;\n}\n\n/**\n * A structured representation of a stop reason that signifies a token limit was reached before the model could naturally\n * complete.\n */\nexport interface MaxTokensFinishDetailsOutput extends ChatFinishDetailsOutputParent {\n  /** The object type, which is always 'max_tokens' for this object. */\n  type: \"max_tokens\";\n}\n\n/**\n * Represents the output results of Azure enhancements to chat completions, as configured via the matching input provided\n * in the request.\n */\nexport interface AzureChatEnhancementsOutput {\n  /** The grounding enhancement that returns the bounding box of the objects detected in the image. */\n  grounding?: AzureGroundingEnhancementOutput;\n}\n\n/** The grounding enhancement that returns the bounding box of the objects detected in the image. */\nexport interface AzureGroundingEnhancementOutput {\n  /** The lines of text detected by the grounding enhancement. */\n  lines: Array<AzureGroundingEnhancementLineOutput>;\n}\n\n/** A content line object consisting of an adjacent sequence of content elements, such as words and selection marks. */\nexport interface AzureGroundingEnhancementLineOutput {\n  /** The text within the line. */\n  text: string;\n  /** An array of spans that represent detected objects and its bounding box information. */\n  spans: Array<AzureGroundingEnhancementLineSpanOutput>;\n}\n\n/** A span object that represents a detected object and its bounding box information. */\nexport interface AzureGroundingEnhancementLineSpanOutput {\n  /** The text content of the span that represents the detected object. */\n  text: string;\n  /**\n   * The character offset within the text where the span begins. This offset is defined as the position of the first\n   * character of the span, counting from the start of the text as Unicode codepoints.\n   */\n  offset: number;\n  /** The length of the span in characters, measured in Unicode codepoints. */\n  length: number;\n  /** An array of objects representing points in the polygon that encloses the detected object. */\n  polygon: Array<AzureGroundingEnhancementCoordinatePointOutput>;\n}\n\n/** A representation of a single polygon point as used by the Azure grounding enhancement. */\nexport interface AzureGroundingEnhancementCoordinatePointOutput {\n  /** The x-coordinate (horizontal axis) of the point. */\n  x: number;\n  /** The y-coordinate (vertical axis) of the point. */\n  y: number;\n}\n\n/** The result of a successful image generation operation. */\nexport interface ImageGenerationsOutput {\n  /**\n   * A timestamp representing when this operation was started.\n   * Expressed in seconds since the Unix epoch of 1970-01-01T00:00:00+0000.\n   */\n  created: number;\n  /** The images generated by the operation. */\n  data: Array<ImageGenerationDataOutput>;\n}\n\n/**\n * A representation of a single generated image, provided as either base64-encoded data or as a URL from which the image\n * may be retrieved.\n */\nexport interface ImageGenerationDataOutput {\n  /** The URL that provides temporary access to download the generated image. */\n  url?: string;\n  /** The complete data for an image, represented as a base64-encoded string. */\n  b64_json?: string;\n  /**\n   * The final prompt used by the model to generate the image.\n   * Only provided with dall-3-models and only when revisions were made to the prompt.\n   */\n  revised_prompt?: string;\n}\n\n/**\n * Representation of the response data from an embeddings request.\n * Embeddings measure the relatedness of text strings and are commonly used for search, clustering,\n * recommendations, and other similar scenarios.\n */\nexport interface EmbeddingsOutput {\n  /** Embedding values for the prompts submitted in the request. */\n  data: Array<EmbeddingItemOutput>;\n  /** Usage counts for tokens input using the embeddings API. */\n  usage: EmbeddingsUsageOutput;\n}\n\n/** Representation of a single embeddings relatedness comparison. */\nexport interface EmbeddingItemOutput {\n  /**\n   * List of embeddings value for the input prompt. These represent a measurement of the\n   * vector-based relatedness of the provided input.\n   */\n  embedding: number[];\n  /** Index of the prompt to which the EmbeddingItem corresponds. */\n  index: number;\n}\n\n/** Measurement of the amount of tokens used in this request and response. */\nexport interface EmbeddingsUsageOutput {\n  /** Number of tokens sent in the original request. */\n  prompt_tokens: number;\n  /** Total number of tokens transacted in this request/response. */\n  total_tokens: number;\n}\n\n/** A polling status update or final response payload for an image operation. */\nexport interface BatchImageGenerationOperationResponseOutput {\n  /** The ID of the operation. */\n  id: string;\n  /** A timestamp when this job or item was created (in unix epochs). */\n  created: number;\n  /** A timestamp when this operation and its associated images expire and will be deleted (in unix epochs). */\n  expires?: number;\n  /** The result of the operation if the operation succeeded. */\n  result?: ImageGenerationsOutput;\n  /**\n   * The status of the operation\n   *\n   * Possible values: notRunning, running, succeeded, canceled, failed\n   */\n  status: string;\n  /** The error if the operation failed. */\n  error?: ErrorModel;\n}\n\n/** Represents the request data used to generate images. */\nexport interface ImageGenerationOptionsOutput {\n  /**\n   * The model name or Azure OpenAI model deployment name to use for image generation. If not specified, dall-e-2 will be\n   * inferred as a default.\n   */\n  model?: string;\n  /** A description of the desired images. */\n  prompt: string;\n  /**\n   * The number of images to generate.\n   * Dall-e-2 models support values between 1 and 10.\n   * Dall-e-3 models only support a value of 1.\n   */\n  n?: number;\n  /**\n   * The desired dimensions for generated images.\n   * Dall-e-2 models support 256x256, 512x512, or 1024x1024.\n   * Dall-e-3 models support 1024x1024, 1792x1024, or 1024x1792.\n   *\n   * Possible values: 256x256, 512x512, 1024x1024, 1792x1024, 1024x1792\n   */\n  size?: string;\n  /**\n   * The format in which image generation response items should be presented.\n   *\n   * Possible values: url, b64_json\n   */\n  response_format?: string;\n  /**\n   * The desired image generation quality level to use.\n   * Only configurable with dall-e-3 models.\n   *\n   * Possible values: standard, hd\n   */\n  quality?: string;\n  /**\n   * The desired image generation style to use.\n   * Only configurable with dall-e-3 models.\n   *\n   * Possible values: natural, vivid\n   */\n  style?: string;\n  /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */\n  user?: string;\n}\n\n/**\n * An abstract representation of a tool call that must be resolved in a subsequent request to perform the requested\n * chat completion.\n */\nexport type ChatCompletionsToolCallOutput = ChatCompletionsFunctionToolCallOutput;\n/** An abstract representation of structured information about why a chat completions response terminated. */\nexport type ChatFinishDetailsOutput = StopFinishDetailsOutput | MaxTokensFinishDetailsOutput;\n"]}