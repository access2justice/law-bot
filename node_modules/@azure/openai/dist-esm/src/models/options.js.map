{"version":3,"file":"options.js","sourceRoot":"","sources":["../../../src/models/options.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/**\n * THIS IS AN AUTO-GENERATED FILE - DO NOT EDIT!\n *\n * Any changes you make here may be lost.\n *\n * If you need to make changes, please do so in the original source file, \\{project-root\\}/sources/custom\n */\n\nimport { OperationOptions } from \"@azure-rest/core-client\";\nimport {\n  AzureExtensionsOptions,\n  ChatCompletionsNamedToolSelection,\n  ChatCompletionsResponseFormat,\n  ChatCompletionsToolDefinition,\n  FunctionCallPreset,\n  FunctionDefinition,\n  FunctionName,\n  ImageGenerationQuality,\n  ImageGenerationResponseFormat,\n  ImageGenerationStyle,\n  ImageSize,\n} from \"./models.js\";\n\nexport interface ClientOpenAIClientGetAudioTranscriptionAsPlainTextOptions\n  extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetAudioTranscriptionAsResponseObjectOptions\n  extends OperationOptions {\n  /** The content type for the operation. Always multipart/form-data for this operation. */\n  contentType?: string;\n}\n\nexport interface ClientOpenAIClientGetAudioTranslationAsPlainTextOptions extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetAudioTranslationAsResponseObjectOptions\n  extends OperationOptions {\n  /** The content type for the operation. Always multipart/form-data for this operation. */\n  contentType?: string;\n}\n\nexport interface ClientOpenAIClientGetCompletionsOptions extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetChatCompletionsOptions extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetChatCompletionsWithAzureExtensionsOptions\n  extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetImageGenerationsOptions extends OperationOptions {}\n\nexport interface ClientOpenAIClientGetEmbeddingsOptions extends OperationOptions {}\n\n/** Represents the request data used to generate images. */\nexport interface GetImagesOptions extends OperationOptions {\n  /**\n   * The number of images to generate.\n   * Dall-e-3 models only support a value of 1.\n   */\n  n?: number;\n  /**\n   * The desired dimensions for generated images.\n   * Dall-e-3 models support 1024x1024, 1792x1024, or 1024x1792.\n   */\n  size?: ImageSize;\n  /** The format in which image generation response items should be presented. */\n  responseFormat?: ImageGenerationResponseFormat;\n  /**\n   * The desired image generation quality level to use.\n   */\n  quality?: ImageGenerationQuality;\n  /**\n   * The desired image generation style to use.\n   */\n  style?: ImageGenerationStyle;\n  /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */\n  user?: string;\n}\n\n/** Options for to custom embeddings request */\nexport interface GetEmbeddingsOptions extends OperationOptions {\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The model name to provide as part of this embeddings request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\n/**\n * The configuration information for a completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface GetCompletionsOptions extends OperationOptions {\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and top_p for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of completions choices that should be generated per provided prompt as part of an\n   * overall completions response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  n?: number;\n  /**\n   * A value that controls the emission of log probabilities for the provided number of most likely\n   * tokens within a completions response.\n   */\n  logprobs?: number;\n  /**\n   * A value specifying whether completions responses should include input prompts as prefixes to\n   * their generated output.\n   */\n  echo?: boolean;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /**\n   * A value that controls how many completions will be internally generated prior to response\n   * formulation.\n   * When used together with n, best_of controls the number of candidate completions and must be\n   * greater than n.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for max_tokens and stop.\n   */\n  bestOf?: number;\n}\n\n/**\n * This module contains models that we want to live side-by-side with the\n * corresponding generated models. This is useful for providing customer-facing\n * models that have different names/types than the generated models.\n */\n\nexport interface GetChatCompletionsOptions extends OperationOptions {\n  /** A list of functions the model may generate JSON inputs for. */\n  functions?: FunctionDefinition[];\n  /**\n   * Controls how the model responds to function calls. \"none\" means the model does not call a function,\n   * and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.\n   *  Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.\n   *  \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n   */\n  functionCall?: FunctionCallPreset | FunctionName;\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of chat completions choices that should be generated for a chat completions\n   * response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for maxTokens and stop.\n   */\n  n?: number;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /**\n   * If specified, the system will make a best effort to sample deterministically such that repeated requests with the\n   * same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the\n   * system_fingerprint response parameter to monitor changes in the backend.\"\n   */\n  seed?: number;\n  /** An object specifying the format that the model must output. Used to enable JSON mode. */\n  responseFormat?: ChatCompletionsResponseFormat;\n  /** The available tool definitions that the chat completions request can use, including caller-defined functions. */\n  tools?: ChatCompletionsToolDefinition[];\n  /** If specified, the model will configure which of the provided tools it can use for the chat completions response. */\n  toolChoice?: ChatCompletionsNamedToolSelection;\n  /**\n   *   The configuration entries for Azure OpenAI chat extensions that use them.\n   *   This additional specification is only compatible with Azure OpenAI.\n   */\n  azureExtensionOptions?: AzureExtensionsOptions;\n}\n"]}