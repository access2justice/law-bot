{"version":3,"file":"models.js","sourceRoot":"","sources":["../../../src/models/models.ts"],"names":[],"mappings":"AAAA,uCAAuC;AACvC,kCAAkC","sourcesContent":["// Copyright (c) Microsoft Corporation.\n// Licensed under the MIT license.\n\n/**\n * THIS IS AN AUTO-GENERATED FILE - DO NOT EDIT!\n *\n * Any changes you make here may be lost.\n *\n * If you need to make changes, please do so in the original source file, \\{project-root\\}/sources/custom\n */\n\nimport { ErrorModel } from \"@azure-rest/core-client\";\n\n/** The configuration information for an audio transcription request. */\nexport interface AudioTranscriptionOptions {\n  /**\n   * The audio data to transcribe. This must be the binary content of a file in one of the supported media formats:\n   *  flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n   */\n  file: Uint8Array;\n  /** The optional filename or descriptive identifier to associate with with the audio data. */\n  filename?: string;\n  /** The requested format of the transcription response data, which will influence the content and detail of the result. */\n  responseFormat?: AudioTranscriptionFormat;\n  /**\n   * The primary spoken language of the audio data to be transcribed, supplied as a two-letter ISO-639-1 language code\n   * such as 'en' or 'fr'.\n   * Providing this known input language is optional but may improve the accuracy and/or latency of transcription.\n   */\n  language?: string;\n  /**\n   * An optional hint to guide the model's style or continue from a prior audio segment. The written language of the\n   * prompt should match the primary spoken language of the audio data.\n   */\n  prompt?: string;\n  /**\n   * The sampling temperature, between 0 and 1.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   * If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n  /** The model to use for this transcription request. */\n  model?: string;\n}\n\n/** Result information for an operation that transcribed spoken audio into written text. */\nexport interface AudioTranscription {\n  /** The transcribed text for the provided audio data. */\n  text: string;\n  /** The label that describes which operation type generated the accompanying response data. */\n  task?: AudioTaskLabel;\n  /**\n   * The spoken language that was detected in the transcribed audio data.\n   * This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.\n   */\n  language?: string;\n  /** The total duration of the audio processed to produce accompanying transcription information. */\n  duration?: number;\n  /** A collection of information about the timing, probabilities, and other detail of each processed audio segment. */\n  segments?: AudioTranscriptionSegment[];\n}\n\n/**\n * Extended information about a single segment of transcribed audio data.\n * Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur between words but not\n * necessarily sentences.\n */\nexport interface AudioTranscriptionSegment {\n  /** The 0-based index of this segment within a transcription. */\n  id: number;\n  /** The time at which this segment started relative to the beginning of the transcribed audio. */\n  start: number;\n  /** The time at which this segment ended relative to the beginning of the transcribed audio. */\n  end: number;\n  /** The transcribed text that was part of this audio segment. */\n  text: string;\n  /** The temperature score associated with this audio segment. */\n  temperature: number;\n  /** The average log probability associated with this audio segment. */\n  avgLogprob: number;\n  /** The compression ratio of this audio segment. */\n  compressionRatio: number;\n  /** The probability of no speech detection within this audio segment. */\n  noSpeechProb: number;\n  /** The token IDs matching the transcribed text in this audio segment. */\n  tokens: number[];\n  /**\n   * The seek position associated with the processing of this audio segment.\n   * Seek positions are expressed as hundredths of seconds.\n   * The model may process several segments from a single seek position, so while the seek position will never represent\n   * a later time than the segment's start, the segment's start may represent a significantly later time than the\n   * segment's associated seek position.\n   */\n  seek: number;\n}\n\n/** The configuration information for an audio translation request. */\nexport interface AudioTranslationOptions {\n  /**\n   * The audio data to translate. This must be the binary content of a file in one of the supported media formats:\n   *  flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n   */\n  file: Uint8Array;\n  /** The optional filename or descriptive identifier to associate with with the audio data. */\n  filename?: string;\n  /** The requested format of the translation response data, which will influence the content and detail of the result. */\n  responseFormat?: AudioTranslationFormat;\n  /**\n   * An optional hint to guide the model's style or continue from a prior audio segment. The written language of the\n   * prompt should match the primary spoken language of the audio data.\n   */\n  prompt?: string;\n  /**\n   * The sampling temperature, between 0 and 1.\n   * Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n   * If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n  /** The model to use for this translation request. */\n  model?: string;\n}\n\n/** Result information for an operation that translated spoken audio into written text. */\nexport interface AudioTranslation {\n  /** The translated text for the provided audio data. */\n  text: string;\n  /** The label that describes which operation type generated the accompanying response data. */\n  task?: AudioTaskLabel;\n  /**\n   * The spoken language that was detected in the translated audio data.\n   * This is expressed as a two-letter ISO-639-1 language code like 'en' or 'fr'.\n   */\n  language?: string;\n  /** The total duration of the audio processed to produce accompanying translation information. */\n  duration?: number;\n  /** A collection of information about the timing, probabilities, and other detail of each processed audio segment. */\n  segments?: AudioTranslationSegment[];\n}\n\n/**\n * Extended information about a single segment of translated audio data.\n * Segments generally represent roughly 5-10 seconds of speech. Segment boundaries typically occur between words but not\n * necessarily sentences.\n */\nexport interface AudioTranslationSegment {\n  /** The 0-based index of this segment within a translation. */\n  id: number;\n  /** The time at which this segment started relative to the beginning of the translated audio. */\n  start: number;\n  /** The time at which this segment ended relative to the beginning of the translated audio. */\n  end: number;\n  /** The translated text that was part of this audio segment. */\n  text: string;\n  /** The temperature score associated with this audio segment. */\n  temperature: number;\n  /** The average log probability associated with this audio segment. */\n  avgLogprob: number;\n  /** The compression ratio of this audio segment. */\n  compressionRatio: number;\n  /** The probability of no speech detection within this audio segment. */\n  noSpeechProb: number;\n  /** The token IDs matching the translated text in this audio segment. */\n  tokens: number[];\n  /**\n   * The seek position associated with the processing of this audio segment.\n   * Seek positions are expressed as hundredths of seconds.\n   * The model may process several segments from a single seek position, so while the seek position will never represent\n   * a later time than the segment's start, the segment's start may represent a significantly later time than the\n   * segment's associated seek position.\n   */\n  seek: number;\n}\n\n/**\n * The configuration information for a completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface CompletionsOptions {\n  /** The prompts to generate completions from. */\n  prompt: string[];\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of completions choices that should be generated per provided prompt as part of an\n   * overall completions response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for maxTokens and stop.\n   */\n  n?: number;\n  /**\n   * A value that controls the emission of log probabilities for the provided number of most likely\n   * tokens within a completions response.\n   */\n  logprobs?: number;\n  /**\n   * A value specifying whether completions responses should include input prompts as prefixes to\n   * their generated output.\n   */\n  echo?: boolean;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /**\n   * A value that controls how many completions will be internally generated prior to response\n   * formulation.\n   * When used together with n, bestOf controls the number of candidate completions and must be\n   * greater than n.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for maxTokens and stop.\n   */\n  bestOf?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n}\n\n/** Information about filtered content severity level and if it has been filtered or not. */\nexport interface ContentFilterResult {\n  /** Ratings for the intensity and risk level of filtered content. */\n  severity: ContentFilterSeverity;\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n}\n\n/** Represents the outcome of a detection operation performed by content filtering. */\nexport interface ContentFilterDetectionResult {\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n  /** A value indicating whether detection occurred, irrespective of severity or whether the content was filtered. */\n  detected: boolean;\n}\n\n/** Represents the outcome of an evaluation against a custom blocklist as performed by content filtering. */\nexport interface ContentFilterBlocklistIdResult {\n  /** The ID of the custom blocklist evaluated. */\n  id: string;\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n}\n\n/** Represents the outcome of a detection operation against protected resources as performed by content filtering. */\nexport interface ContentFilterCitedDetectionResult {\n  /** A value indicating whether or not the content has been filtered. */\n  filtered: boolean;\n  /** A value indicating whether detection occurred, irrespective of severity or whether the content was filtered. */\n  detected: boolean;\n  /** The internet location associated with the detection. */\n  url?: string;\n  /** The license description associated with the detection. */\n  license: string;\n}\n\n/** Representation of a log probabilities model for a completions generation. */\nexport interface CompletionsLogProbabilityModel {\n  /** The textual forms of tokens evaluated in this probability model. */\n  tokens: string[];\n  /** A collection of log probability values for the tokens in this completions data. */\n  tokenLogprobs: (number | null)[];\n  /** A mapping of tokens to maximum log probability values in this completions data. */\n  topLogprobs: Record<string, number | null>[];\n  /** The text offsets associated with tokens in this completions data. */\n  textOffset: number[];\n}\n\n/** Representation of a log probabilities model for a completions generation. */\nexport interface CompletionsLogProbabilityModel {\n  /** The textual forms of tokens evaluated in this probability model. */\n  tokens: string[];\n  /** A collection of log probability values for the tokens in this completions data. */\n  tokenLogprobs: (number | null)[];\n  /** A mapping of tokens to maximum log probability values in this completions data. */\n  topLogprobs: Record<string, number | null>[];\n  /** The text offsets associated with tokens in this completions data. */\n  textOffset: number[];\n}\n\n/**\n * Representation of the token counts processed for a completions request.\n * Counts consider all tokens across prompts, choices, choice alternates, bestOf generations, and\n * other consumers.\n */\nexport interface CompletionsUsage {\n  /** The number of tokens generated across all completions emissions. */\n  completionTokens: number;\n  /** The number of tokens in the provided prompts for the completions request. */\n  promptTokens: number;\n  /** The total number of tokens processed for the completions request and response. */\n  totalTokens: number;\n}\n\n/**\n * The configuration information for a chat completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface ChatCompletionsOptions {\n  /**\n   * The collection of context messages associated with this chat completions request.\n   * Typical usage begins with a chat message for the System role that provides instructions for\n   * the behavior of the assistant, followed by alternating messages between the User and\n   * Assistant roles.\n   */\n  messages: ChatRequestMessage[];\n  /** A list of functions the model may generate JSON inputs for. */\n  functions?: FunctionDefinition[];\n  /**\n   * Controls how the model responds to function calls. \"none\" means the model does not call a function,\n   * and responds to the end-user. \"auto\" means the model can pick between an end-user or calling a function.\n   *  Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.\n   *  \"none\" is the default when no functions are present. \"auto\" is the default if functions are present.\n   */\n  functionCall?: FunctionCallPreset | FunctionName;\n  /** The maximum number of tokens to generate. */\n  maxTokens?: number;\n  /**\n   * The sampling temperature to use that controls the apparent creativity of generated completions.\n   * Higher values will make output more random while lower values will make results more focused\n   * and deterministic.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  temperature?: number;\n  /**\n   * An alternative to sampling with temperature called nucleus sampling. This value causes the\n   * model to consider the results of tokens with the provided probability mass. As an example, a\n   * value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be\n   * considered.\n   * It is not recommended to modify temperature and topP for the same completions request as the\n   * interaction of these two settings is difficult to predict.\n   */\n  topP?: number;\n  /**\n   * A map between GPT token IDs and bias scores that influences the probability of specific tokens\n   * appearing in a completions response. Token IDs are computed via external tokenizer tools, while\n   * bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to\n   * a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias\n   * score varies by model.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The number of chat completions choices that should be generated for a chat completions\n   * response.\n   * Because this setting can generate many completions, it may quickly consume your token quota.\n   * Use carefully and ensure reasonable settings for maxTokens and stop.\n   */\n  n?: number;\n  /** A collection of textual sequences that will end completions generation. */\n  stop?: string[];\n  /**\n   * A value that influences the probability of generated tokens appearing based on their existing\n   * presence in generated text.\n   * Positive values will make tokens less likely to appear when they already exist and increase the\n   * model's likelihood to output new topics.\n   */\n  presencePenalty?: number;\n  /**\n   * A value that influences the probability of generated tokens appearing based on their cumulative\n   * frequency in generated text.\n   * Positive values will make tokens less likely to appear as their frequency increases and\n   * decrease the likelihood of the model repeating the same statements verbatim.\n   */\n  frequencyPenalty?: number;\n  /** A value indicating whether chat completions should be streamed for this request. */\n  stream?: boolean;\n  /**\n   * The model name to provide as part of this completions request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n  /**\n   *   The configuration entries for Azure OpenAI chat extensions that use them.\n   *   This additional specification is only compatible with Azure OpenAI.\n   */\n  dataSources?: AzureChatExtensionConfiguration[];\n  /** If provided, the configuration options for available Azure OpenAI chat enhancements. */\n  enhancements?: AzureChatEnhancementConfiguration;\n  /**\n   * If specified, the system will make a best effort to sample deterministically such that repeated requests with the\n   * same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the\n   * systemFingerprint response parameter to monitor changes in the backend.\"\n   */\n  seed?: number;\n  /** An object specifying the format that the model must output. Used to enable JSON mode. */\n  responseFormat?: ChatCompletionsResponseFormat;\n  /** The available tool definitions that the chat completions request can use, including caller-defined functions. */\n  tools?: ChatCompletionsToolDefinition[];\n  /** If specified, the model will configure which of the provided tools it can use for the chat completions response. */\n  toolChoice?: ChatCompletionsNamedToolSelection;\n}\n\n/** A representation of a structured content item within a chat message. */\nexport type ChatMessageContentItem = ChatMessageTextContentItem | ChatMessageImageContentItem;\n\n/** A structured chat content item containing plain text. */\nexport interface ChatMessageTextContentItem {\n  /** The discriminated object type: always 'text' for this type. */\n  type: \"text\";\n  /** The content of the message. */\n  text: string;\n}\n\n/** A structured chat content item containing an image reference. */\nexport interface ChatMessageImageContentItem {\n  /** The discriminated object type: always 'image_url' for this type. */\n  type: \"image_url\";\n  /** An internet location, which must be accessible to the model,from which the image may be retrieved. */\n  imageUrl: ChatMessageImageUrl;\n}\n\n/** An internet location from which the model may retrieve an image. */\nexport interface ChatMessageImageUrl {\n  /** The URL of the image. */\n  url: string;\n  /**\n   * The evaluation quality setting to use, which controls relative prioritization of speed, token consumption, and\n   * accuracy.\n   *\n   * Possible values: auto, low, high\n   */\n  detail?: ChatMessageImageDetailLevel;\n}\n\n/** A representation of the possible image detail levels for image-based chat completions message content. */\n/** \"auto\", \"low\", \"high\" */\nexport type ChatMessageImageDetailLevel = \"auto\" | \"low\" | \"high\";\n\n/**\n * A tool call to a function tool, issued by the model in evaluation of a configured function tool, that represents\n * a function invocation needed for a subsequent chat completions request to resolve.\n */\nexport interface ChatCompletionsFunctionToolCall {\n  /** The type of tool call, in this case always 'function'. */\n  type: \"function\";\n  /** The details of the function invocation requested by the tool call. */\n  function: FunctionCall;\n  /** The ID of the tool call. */\n  id: string;\n}\n\n/**\n * A representation of a tool call that must be resolved in a subsequent request to perform the requested\n * chat completion.\n */\nexport type ChatCompletionsToolCall = ChatCompletionsFunctionToolCall;\n\n/** The name and arguments of a function that should be called, as generated by the model. */\nexport interface FunctionCall {\n  /** The name of the function to call. */\n  name: string;\n  /**\n   * The arguments to call the function with, as generated by the model in JSON format.\n   * Note that the model does not always generate valid JSON, and may hallucinate parameters\n   * not defined by your function schema. Validate the arguments in your code before calling\n   * your function.\n   */\n  arguments: string;\n}\n\n/** The definition of a caller-specified function that chat completions may invoke in response to matching user input. */\nexport interface FunctionDefinition {\n  /** The name of the function to be called. */\n  name: string;\n  /**\n   * A description of what the function does. The model will use this description when selecting the function and\n   * interpreting its parameters.\n   */\n  description?: string;\n  /** The parameters the functions accepts, described as a JSON Schema object. */\n  parameters?: Record<string, any>;\n}\n\n/**\n * A structure that specifies the exact name of a specific, request-provided function to use when processing a chat\n * completions operation.\n */\nexport interface FunctionName {\n  /** The name of the function to call. */\n  name: string;\n}\n\n/** Optional settings to control how fields are processed when using a configured Azure Cognitive Search resource. */\nexport interface AzureCognitiveSearchIndexFieldMappingOptions {\n  /** The name of the index field to use as a title. */\n  titleField?: string;\n  /** The name of the index field to use as a URL. */\n  urlField?: string;\n  /** The name of the index field to use as a filepath. */\n  filepathField?: string;\n  /** The names of index fields that should be treated as content. */\n  contentFields?: string[];\n  /** The separator pattern that content fields should use. */\n  contentFieldsSeparator?: string;\n  /** The names of fields that represent vector data. */\n  vectorFields?: string[];\n  /** The names of fields that represent image vector data. */\n  imageVectorFields?: string[];\n}\n\n/** Optional settings to control how fields are processed when using a configured Azure Cosmos DB resource. */\nexport interface AzureCosmosDBFieldMappingOptions {\n  /** The names of fields that represent vector data. */\n  vectorFields: string[];\n}\n\n/** Optional settings to control how fields are processed when using a configured Elasticsearch® resource. */\nexport interface ElasticsearchIndexFieldMappingOptions {\n  /** The name of the index field to use as a title. */\n  titleField?: string;\n  /** The name of the index field to use as a URL. */\n  urlField?: string;\n  /** The name of the index field to use as a filepath. */\n  filepathField?: string;\n  /** The names of index fields that should be treated as content. */\n  contentFields?: string[];\n  /** The separator pattern that content fields should use. */\n  contentFieldsSeparator?: string;\n  /** The names of fields that represent vector data. */\n  vectorFields?: string[];\n}\n\n/** Optional settings to control how fields are processed when using a configured Pinecone resource. */\nexport interface PineconeFieldMappingOptions {\n  /** The name of the index field to use as a title. */\n  titleField?: string;\n  /** The name of the index field to use as a URL. */\n  urlField?: string;\n  /** The name of the index field to use as a filepath. */\n  filepathField?: string;\n  /** The names of index fields that should be treated as content. */\n  contentFields?: string[];\n  /** The separator pattern that content fields should use. */\n  contentFieldsSeparator?: string;\n  /** The names of fields that represent vector data. */\n  vectorFields?: string[];\n  /** The names of fields that represent image vector data. */\n  imageVectorFields?: string[];\n}\n\n/** A representation of the available Azure OpenAI enhancement configurations. */\nexport interface AzureChatEnhancementConfiguration {\n  /** A representation of the available options for the Azure OpenAI grounding enhancement. */\n  grounding?: AzureChatGroundingEnhancementConfiguration;\n  /** A representation of the available options for the Azure OpenAI optical character recognition (OCR) enhancement. */\n  ocr?: AzureChatOCREnhancementConfiguration;\n}\n\n/** A representation of the available options for the Azure OpenAI grounding enhancement. */\nexport interface AzureChatGroundingEnhancementConfiguration {\n  /** Specifies whether the enhancement is enabled. */\n  enabled: boolean;\n}\n\n/** A representation of the available options for the Azure OpenAI optical character recognition (OCR) enhancement. */\nexport interface AzureChatOCREnhancementConfiguration {\n  /** Specifies whether the enhancement is enabled. */\n  enabled: boolean;\n}\n\n/** A representation of a tool that can be used by the model to improve a chat completions response. */\nexport type ChatCompletionsToolDefinition = ChatCompletionsFunctionToolDefinition;\n\n/** The definition information for a chat completions function tool that can call a function in response to a tool call. */\nexport interface ChatCompletionsFunctionToolDefinition {\n  /** The object name, which is always 'function'. */\n  type: \"function\";\n  /** The function definition details for the function tool. */\n  function: FunctionDefinition;\n}\n\n/** A representation of an explicit, named tool selection to use for a chat completions request. */\nexport type ChatCompletionsNamedToolSelection =\n  | ChatCompletionsToolSelectionPreset\n  | ChatCompletionsNamedFunctionToolSelection;\n\n/** A tool selection of a specific, named function tool that will limit chat completions to using the named function. */\nexport interface ChatCompletionsNamedFunctionToolSelection {\n  /** The object type, which is always 'function'. */\n  type: \"function\";\n  /** Specifies a tool the model should use. Used to force the model to call a specific function. */\n  function: {\n    /** The name of the function that should be called. */\n    name: string;\n  };\n}\n\n/** A representation of a chat message as received in a response. */\nexport interface ChatResponseMessage {\n  /** The chat role associated with the message. */\n  role: ChatRole;\n  /** The content of the message. */\n  content: string | null;\n  /**\n   * The tool calls that must be resolved and have their outputs appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  toolCalls: ChatCompletionsToolCall[];\n  /**\n   * The function call that must be resolved and have its output appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  functionCall?: FunctionCall;\n  /**\n   * If Azure OpenAI chat extensions are configured, this array represents the incremental steps performed by those\n   * extensions while processing the chat completions request.\n   */\n  context?: AzureChatExtensionsMessageContext;\n}\n\n/**\n *   A representation of the additional context information available when Azure OpenAI chat extensions are involved\n *   in the generation of a corresponding chat completions response. This context information is only populated when\n *   using an Azure OpenAI request configured to use a matching extension.\n */\nexport interface AzureChatExtensionsMessageContext {\n  /**\n   *   The contextual message payload associated with the Azure chat extensions used for a chat completions request.\n   *   These messages describe the data source retrievals, plugin invocations, and other intermediate steps taken in the\n   *   course of generating a chat completions response that was augmented by capabilities from Azure OpenAI chat\n   *   extensions.\n   */\n  messages?: ChatResponseMessage[];\n}\n\n/** A structured representation of a stop reason that signifies natural termination by the model. */\nexport interface StopFinishDetails {\n  /** The object type, which is always 'stop' for this object. */\n  type: \"stop\";\n  /** The token sequence that the model terminated with. */\n  stop: string;\n}\n\n/**\n * A structured representation of a stop reason that signifies a token limit was reached before the model could naturally\n * complete.\n */\nexport interface MaxTokensFinishDetails {\n  /** The object type, which is always 'max_tokens' for this object. */\n  type: \"max_tokens\";\n}\n\n/** Structured information about why a chat completions response terminated. */\nexport type ChatFinishDetails = StopFinishDetails | MaxTokensFinishDetails;\n\n/**\n * Represents the output results of Azure enhancements to chat completions, as configured via the matching input provided\n * in the request.\n */\nexport interface AzureChatEnhancements {\n  /** The grounding enhancement that returns the bounding box of the objects detected in the image. */\n  grounding?: AzureGroundingEnhancement;\n}\n\n/** The grounding enhancement that returns the bounding box of the objects detected in the image. */\nexport interface AzureGroundingEnhancement {\n  /** The lines of text detected by the grounding enhancement. */\n  lines: AzureGroundingEnhancementLine[];\n}\n\n/** A content line object consisting of an adjacent sequence of content elements, such as words and selection marks. */\nexport interface AzureGroundingEnhancementLine {\n  /** The text within the line. */\n  text: string;\n  /** An array of spans that represent detected objects and its bounding box information. */\n  spans: AzureGroundingEnhancementLineSpan[];\n}\n\n/** A span object that represents a detected object and its bounding box information. */\nexport interface AzureGroundingEnhancementLineSpan {\n  /** The text content of the span that represents the detected object. */\n  text: string;\n  /**\n   * The character offset within the text where the span begins. This offset is defined as the position of the first\n   * character of the span, counting from the start of the text as Unicode codepoints.\n   */\n  offset: number;\n  /** The length of the span in characters, measured in Unicode codepoints. */\n  length: number;\n  /** An array of objects representing points in the polygon that encloses the detected object. */\n  polygon: AzureGroundingEnhancementCoordinatePoint[];\n}\n\n/** A representation of a single polygon point as used by the Azure grounding enhancement. */\nexport interface AzureGroundingEnhancementCoordinatePoint {\n  /** The x-coordinate (horizontal axis) of the point. */\n  x: number;\n  /** The y-coordinate (vertical axis) of the point. */\n  y: number;\n}\n\n/** Represents the request data used to generate images. */\nexport interface ImageGenerationOptions {\n  /**\n   * The model name or Azure OpenAI model deployment name to use for image generation. If not specified, dall-e-2 will be\n   * inferred as a default.\n   */\n  model?: string;\n  /** A description of the desired images. */\n  prompt: string;\n  /**\n   * The number of images to generate.\n   * Dall-e-2 models support values between 1 and 10.\n   * Dall-e-3 models only support a value of 1.\n   */\n  n?: number;\n  /**\n   * The desired dimensions for generated images.\n   * Dall-e-2 models support 256x256, 512x512, or 1024x1024.\n   * Dall-e-3 models support 1024x1024, 1792x1024, or 1024x1792.\n   */\n  size?: ImageSize;\n  /** The format in which image generation response items should be presented. */\n  responseFormat?: ImageGenerationResponseFormat;\n  /**\n   * The desired image generation quality level to use.\n   * Only configurable with dall-e-3 models.\n   */\n  quality?: ImageGenerationQuality;\n  /**\n   * The desired image generation style to use.\n   * Only configurable with dall-e-3 models.\n   */\n  style?: ImageGenerationStyle;\n  /** A unique identifier representing your end-user, which can help to monitor and detect abuse. */\n  user?: string;\n}\n\n/** The result of a successful image generation operation. */\nexport interface ImageGenerations {\n  /**\n   * A timestamp representing when this operation was started.\n   * Expressed in seconds since the Unix epoch of 1970-01-01T00:00:00+0000.\n   */\n  created: Date;\n  /** The images generated by the operation. */\n  data: ImageGenerationData[];\n}\n\n/**\n * A representation of a single generated image, provided as either base64-encoded data or as a URL from which the image\n * may be retrieved.\n */\nexport interface ImageGenerationData {\n  /** The URL that provides temporary access to download the generated image. */\n  url?: string;\n  /** The complete data for an image, represented as a base64-encoded string. */\n  base64Data?: string;\n  /**\n   * The final prompt used by the model to generate the image.\n   * Only provided with dall-3-models and only when revisions were made to the prompt.\n   */\n  revisedPrompt?: string;\n}\n\n/**\n * The configuration information for an embeddings request.\n * Embeddings measure the relatedness of text strings and are commonly used for search, clustering,\n * recommendations, and other similar scenarios.\n */\nexport interface EmbeddingsOptions {\n  /**\n   * An identifier for the caller or end user of the operation. This may be used for tracking\n   * or rate-limiting purposes.\n   */\n  user?: string;\n  /**\n   * The model name to provide as part of this embeddings request.\n   * Not applicable to Azure OpenAI, where deployment information should be included in the Azure\n   * resource URI that's connected to.\n   */\n  model?: string;\n  /**\n   * Input texts to get embeddings for, encoded as a an array of strings.\n   * Each input must not exceed 2048 tokens in length.\n   *\n   * Unless you are embedding code, we suggest replacing newlines (\\\\n) in your input with a single space,\n   * as we have observed inferior results when newlines are present.\n   */\n  input: string[];\n}\n\n/**\n * Representation of the response data from an embeddings request.\n * Embeddings measure the relatedness of text strings and are commonly used for search, clustering,\n * recommendations, and other similar scenarios.\n */\nexport interface Embeddings {\n  /** Embedding values for the prompts submitted in the request. */\n  data: EmbeddingItem[];\n  /** Usage counts for tokens input using the embeddings API. */\n  usage: EmbeddingsUsage;\n}\n\n/** Representation of a single embeddings relatedness comparison. */\nexport interface EmbeddingItem {\n  /**\n   * List of embeddings value for the input prompt. These represent a measurement of the\n   * vector-based relatedness of the provided input.\n   */\n  embedding: number[];\n  /** Index of the prompt to which the EmbeddingItem corresponds. */\n  index: number;\n}\n\n/** Measurement of the amount of tokens used in this request and response. */\nexport interface EmbeddingsUsage {\n  /** Number of tokens sent in the original request. */\n  promptTokens: number;\n  /** Total number of tokens transacted in this request/response. */\n  totalTokens: number;\n}\n\n/** Defines available options for the underlying response format of output transcription information. */\n/** \"json\", \"verbose_json\", \"text\", \"srt\", \"vtt\" */\nexport type AudioTranscriptionFormat = string;\n/** Defines the possible descriptors for available audio operation responses. */\n/** \"transcribe\", \"translate\" */\nexport type AudioTaskLabel = string;\n/** Defines available options for the underlying response format of output translation information. */\n/** \"json\", \"verbose_json\", \"text\", \"srt\", \"vtt\" */\nexport type AudioTranslationFormat = string;\n/** Ratings for the intensity and risk level of harmful content. */\n/** \"safe\", \"low\", \"medium\", \"high\" */\nexport type ContentFilterSeverity = string;\n/** Representation of the manner in which a completions response concluded. */\n/** \"stop\", \"length\", \"content_filter\", \"function_call\", \"tool_calls\" */\nexport type CompletionsFinishReason = string;\n/** A description of the intended purpose of a message within a chat completions interaction. */\n/** \"system\", \"assistant\", \"user\", \"function\", \"tool\" */\nexport type ChatRole = string;\n/**\n * The collection of predefined behaviors for handling request-provided function information in a chat completions\n * operation.\n */\n/** \"auto\", \"none\" */\nexport type FunctionCallPreset = \"auto\" | \"none\";\n/**\n *   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\n *   completions request that should use Azure OpenAI chat extensions to augment the response behavior.\n *   The use of this configuration is compatible only with Azure OpenAI.\n */\n/** \"AzureCognitiveSearch\", \"AzureMLIndex\", \"AzureCosmosDB\", \"Elasticsearch\", \"Pinecone\" */\nexport type AzureChatExtensionType = string;\n/** The authentication types supported with Azure OpenAI On Your Data. */\n/** \"APIKey\", \"ConnectionString\", \"KeyAndKeyId\", \"SystemAssignedManagedIdentity\", \"UserAssignedManagedIdentity\" */\nexport type OnYourDataAuthenticationType =\n  | \"APIKey\"\n  | \"ConnectionString\"\n  | \"KeyAndKeyId\"\n  | \"SystemAssignedManagedIdentity\"\n  | \"UserAssignedManagedIdentity\";\n/** The type of Azure Cognitive Search retrieval query that should be executed when using it as an Azure OpenAI chat extension. */\n/** \"simple\", \"semantic\", \"vector\", \"vectorSimpleHybrid\", \"vectorSemanticHybrid\" */\nexport type AzureCognitiveSearchQueryType =\n  | \"simple\"\n  | \"semantic\"\n  | \"vector\"\n  | \"vectorSimpleHybrid\"\n  | \"vectorSemanticHybrid\";\n/**\n * Represents the available sources Azure OpenAI On Your Data can use to configure vectorization of data for use with\n * vector search.\n */\n/** \"Endpoint\", \"DeploymentName\", \"ModelId\" */\nexport type OnYourDataVectorizationSourceType = \"Endpoint\" | \"DeploymentName\" | \"ModelId\";\n/** The type of Elasticsearch® retrieval query that should be executed when using it as an Azure OpenAI chat extension. */\n/** \"simple\", \"vector\" */\nexport type ElasticsearchQueryType = \"simple\" | \"vector\";\n\n/** The standard Chat Completions response format that can freely generate text and is not guaranteed to produce response\ncontent that adheres to a specific schema. */\nexport interface ChatCompletionsTextResponseFormat {\n  /** The object type, which is always 'text' for this object. */\n  type: \"text\";\n}\n/** A response format for Chat Completions that restricts responses to emitting valid JSON objects.\n */\nexport interface ChatCompletionsJsonResponseFormat {\n  /** The object type, which is always 'json_object' for this object. */\n  type: \"json_object\";\n}\n/** The valid response formats Chat Completions can provide. Used to enable JSON mode. */\nexport type ChatCompletionsResponseFormat =\n  | ChatCompletionsTextResponseFormat\n  | ChatCompletionsJsonResponseFormat;\n/** Represents a generic policy for how a chat completions tool may be selected. */\n/** \"auto\", \"none\" */\nexport type ChatCompletionsToolSelectionPreset = \"auto\" | \"none\";\n/** The desired size of generated images. */\n/** \"1024x1024\", \"1792x1024\", \"1024x1792\" */\nexport type ImageSize = \"1024x1024\" | \"1792x1024\" | \"1024x1792\";\n/** The format in which the generated images are returned. */\n/** \"url\", \"b64_json\" */\nexport type ImageGenerationResponseFormat = \"url\" | \"b64_json\";\n/**\n * An image generation configuration that specifies how the model should prioritize quality, cost, and speed.\n * Only configurable with dall-e-3 models.\n */\n/** \"standard\", \"hd\" */\nexport type ImageGenerationQuality = \"standard\" | \"hd\";\n/**\n * An image generation configuration that specifies how the model should incorporate realism and other visual characteristics.\n * Only configurable with dall-e-3 models.\n */\n/** \"natural\", \"vivid\" */\nexport type ImageGenerationStyle = \"natural\" | \"vivid\";\n\n/**\n * The representation of a single prompt completion as part of an overall completions request.\n * Generally, `n` choices are generated per provided prompt with a default value of 1.\n * Token limits and other settings may limit the number of choices generated.\n */\nexport interface Choice {\n  /** The generated text for a given completions prompt. */\n  text: string;\n  /** The ordered index associated with this completions choice. */\n  index: number;\n  /**\n   * Information about the content filtering category (hate, sexual, violence, selfHarm), if it\n   * has been detected, as well as the severity level (very_low, low, medium, high-scale that\n   * determines the intensity and risk level of harmful content) and if it has been filtered or not.\n   */\n  contentFilterResults?: ContentFilterResultsForChoice;\n  /** The log probabilities model for tokens associated with this completions choice. */\n  logprobs: CompletionsLogProbabilityModel | null;\n  /** Reason for finishing */\n  finishReason: CompletionsFinishReason | null;\n}\n\n/**\n * Representation of the response data from a completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface Completions {\n  /** A unique identifier associated with this completions response. */\n  id: string;\n  /**\n   * The first timestamp associated with generation activity for this completions response,\n   * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.\n   */\n  created: Date;\n  /**\n   * Content filtering results for zero or more prompts in the request. In a streaming request,\n   * results for different prompts may arrive at different times or in different orders.\n   */\n  promptFilterResults: ContentFilterResultsForPrompt[];\n  /**\n   * The collection of completions choices associated with this completions response.\n   * Generally, `n` choices are generated per provided prompt with a default value of 1.\n   * Token limits and other settings may limit the number of choices generated.\n   */\n  choices: Choice[];\n  /** Usage information for tokens processed and generated as part of this completions operation. */\n  usage: CompletionsUsage;\n}\n\n/**\n * The representation of a single prompt completion as part of an overall chat completions request.\n * Generally, `n` choices are generated per provided prompt with a default value of 1.\n * Token limits and other settings may limit the number of choices generated.\n */\nexport interface ChatChoice {\n  /** The chat message for a given chat completions prompt. */\n  message?: ChatResponseMessage;\n  /** The ordered index associated with this chat completions choice. */\n  index: number;\n  /** The reason that this chat completions choice completed its generated. */\n  finishReason: CompletionsFinishReason | null;\n  /**\n   * The reason the model stopped generating tokens, together with any applicable details.\n   * This structured representation replaces 'finishReason' for some models.\n   */\n  finishDetails?: ChatFinishDetails;\n  /** The delta message content for a streaming response. */\n  delta?: ChatResponseMessage;\n  /**\n   * Information about the content filtering category (hate, sexual, violence, selfHarm), if it\n   * has been detected, as well as the severity level (very_low, low, medium, high-scale that\n   * determines the intensity and risk level of harmful content) and if it has been filtered or not.\n   */\n  contentFilterResults?: ContentFilterResultsForChoice;\n  /**\n   * Represents the output results of Azure OpenAI enhancements to chat completions, as configured via the matching input\n   * provided in the request. This supplementary information is only available when using Azure OpenAI and only when the\n   * request is configured to use enhancements.\n   */\n  enhancements?: AzureChatEnhancements;\n}\n\n/** Content filtering results for a single prompt in the request. */\nexport interface ContentFilterResultsForPrompt {\n  /** The index of this prompt in the set of prompt results */\n  promptIndex: number;\n  /** Content filtering results for this prompt */\n  contentFilterResults: ContentFilterResultDetailsForPrompt;\n}\n\n/**\n * Representation of the response data from a chat completions request.\n * Completions support a wide variety of tasks and generate text that continues from or \"completes\"\n * provided prompt data.\n */\nexport interface ChatCompletions {\n  /** A unique identifier associated with this chat completions response. */\n  id: string;\n  /**\n   * The first timestamp associated with generation activity for this completions response,\n   * represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.\n   */\n  created: Date;\n  /**\n   * The collection of completions choices associated with this completions response.\n   * Generally, `n` choices are generated per provided prompt with a default value of 1.\n   * Token limits and other settings may limit the number of choices generated.\n   */\n  choices: ChatChoice[];\n  /**\n   * Content filtering results for zero or more prompts in the request. In a streaming request,\n   * results for different prompts may arrive at different times or in different orders.\n   */\n  promptFilterResults: ContentFilterResultsForPrompt[];\n  /**\n   * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that\n   * might impact determinism.\n   */\n  systemFingerprint?: string;\n  /** Usage information for tokens processed and generated as part of this completions operation. */\n  usage?: CompletionsUsage;\n}\n\n/** Information about the content filtering category, if it has been detected. */\nexport type ContentFilterResultDetailsForPrompt =\n  | ContentFilterSuccessResultDetailsForPrompt\n  | ContentFilterErrorResults;\n\n/** Information about the content filtering success result. */\nexport interface ContentFilterSuccessResultDetailsForPrompt {\n  /**\n   * Describes language related to anatomical organs and genitals, romantic relationships,\n   *  acts portrayed in erotic or affectionate terms, physical sexual acts, including\n   *  those portrayed as an assault or a forced sexual violent act against one’s will,\n   *  prostitution, pornography, and abuse.\n   */\n  sexual?: ContentFilterResult;\n  /**\n   * Describes language related to physical actions intended to hurt, injure, damage, or\n   * kill someone or something; describes weapons, etc.\n   */\n  violence?: ContentFilterResult;\n  /**\n   * Describes language attacks or uses that include pejorative or discriminatory language\n   * with reference to a person or identity group on the basis of certain differentiating\n   * attributes of these groups including but not limited to race, ethnicity, nationality,\n   * gender identity and expression, sexual orientation, religion, immigration status, ability\n   * status, personal appearance, and body size.\n   */\n  hate?: ContentFilterResult;\n  /**\n   * Describes language related to physical actions intended to purposely hurt, injure,\n   * or damage one’s body, or kill oneself.\n   */\n  selfHarm?: ContentFilterResult;\n  /**\n   * Describes an error returned if the content filtering system is\n   * down or otherwise unable to complete the operation in time.\n   */\n  error?: undefined;\n  /** Describes whether profanity was detected. */\n  profanity?: ContentFilterDetectionResult;\n  /** Describes detection results against configured custom blocklists. */\n  customBlocklists?: ContentFilterBlocklistIdResult[];\n  /** Whether a jailbreak attempt was detected in the prompt. */\n  jailbreak?: ContentFilterDetectionResult;\n}\n\n/** Information about the content filtering error result. */\nexport interface ContentFilterErrorResults {\n  /**\n   * Describes an error returned if the content filtering system is\n   * down or otherwise unable to complete the operation in time.\n   */\n  error: ErrorModel;\n}\n\n/** Information about content filtering evaluated against generated model output. */\nexport interface ContentFilterSuccessResultsForChoice {\n  /**\n   * Describes language related to anatomical organs and genitals, romantic relationships,\n   *  acts portrayed in erotic or affectionate terms, physical sexual acts, including\n   *  those portrayed as an assault or a forced sexual violent act against one’s will,\n   *  prostitution, pornography, and abuse.\n   */\n  sexual?: ContentFilterResult;\n  /**\n   * Describes language related to physical actions intended to hurt, injure, damage, or\n   * kill someone or something; describes weapons, etc.\n   */\n  violence?: ContentFilterResult;\n  /**\n   * Describes language attacks or uses that include pejorative or discriminatory language\n   * with reference to a person or identity group on the basis of certain differentiating\n   * attributes of these groups including but not limited to race, ethnicity, nationality,\n   * gender identity and expression, sexual orientation, religion, immigration status, ability\n   * status, personal appearance, and body size.\n   */\n  hate?: ContentFilterResult;\n  /**\n   * Describes language related to physical actions intended to purposely hurt, injure,\n   * or damage one’s body, or kill oneself.\n   */\n  selfHarm?: ContentFilterResult;\n  /** Describes whether profanity was detected. */\n  profanity?: ContentFilterDetectionResult;\n  /** Describes detection results against configured custom blocklists. */\n  customBlocklists?: ContentFilterBlocklistIdResult[];\n  /**\n   * Describes an error returned if the content filtering system is\n   * down or otherwise unable to complete the operation in time.\n   */\n  error?: undefined;\n  /** Information about detection of protected text material. */\n  protectedMaterialText?: ContentFilterDetectionResult;\n  /** Information about detection of protected code material. */\n  protectedMaterialCode?: ContentFilterCitedDetectionResult;\n}\n\n/** Information about the content filtering results, if it has been detected. */\nexport type ContentFilterResultsForChoice =\n  | ContentFilterSuccessResultsForChoice\n  | ContentFilterErrorResults;\n\n/**\n * A request chat message containing system instructions that influence how the model will generate a chat completions\n * response.\n */\nexport interface ChatRequestSystemMessage {\n  /** The chat role associated with this message, which is always 'system' for system messages. */\n  role: \"system\";\n  /** The contents of the system message. */\n  content: string;\n  /** An optional name for the participant. */\n  name?: string;\n}\n\n/** A request chat message representing response or action from the assistant. */\nexport interface ChatRequestAssistantMessage {\n  /** The chat role associated with this message, which is always 'assistant' for assistant messages. */\n  role: \"assistant\";\n  /** The content of the message. */\n  content: string | null;\n  /** An optional name for the participant. */\n  name?: string;\n  /**\n   * The tool calls that must be resolved and have their outputs appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  toolCalls?: Array<ChatCompletionsToolCall>;\n  /**\n   * The function call that must be resolved and have its output appended to subsequent input messages for the chat\n   * completions request to resolve as configured.\n   */\n  functionCall?: FunctionCall;\n}\n\n/** A request chat message representing user input to the assistant. */\nexport interface ChatRequestUserMessage {\n  /** The chat role associated with this message, which is always 'user' for user messages. */\n  role: \"user\";\n  /** The contents of the user message, with available input types varying by selected model. */\n  content: string | Array<ChatMessageContentItem>;\n  /** An optional name for the participant. */\n  name?: string;\n}\n\n/** A request chat message representing requested output from a configured tool. */\nexport interface ChatRequestToolMessage {\n  /** The chat role associated with this message, which is always 'tool' for tool messages. */\n  role: \"tool\";\n  /** The content of the message. */\n  content: string | null;\n  /** The ID of the tool call resolved by the provided content. */\n  toolCallId: string;\n}\n\n/** A request chat message representing requested output from a configured function. */\nexport interface ChatRequestFunctionMessage {\n  /** The chat role associated with this message, which is always 'function' for function messages. */\n  role: \"function\";\n  /** The name of the function that was called to produce output. */\n  name: string;\n  /** The output of the function as requested by the function call. */\n  content: string | null;\n}\n\n/** An abstract representation of a chat message as provided in a request. */\nexport type ChatRequestMessage =\n  | ChatRequestSystemMessage\n  | ChatRequestUserMessage\n  | ChatRequestAssistantMessage\n  | ChatRequestToolMessage\n  | ChatRequestFunctionMessage;\n\n/**\n * Options for Azure OpenAI chat extensions.\n */\nexport interface AzureExtensionsOptions {\n  /**\n   *   The configuration entries for Azure OpenAI chat extensions that use them.\n   *   This additional specification is only compatible with Azure OpenAI.\n   */\n  extensions?: AzureChatExtensionConfiguration[];\n  /** If provided, the configuration options for available Azure OpenAI chat enhancements. */\n  enhancements?: AzureChatEnhancementConfiguration;\n}\n\n/**\n * A specific representation of configurable options for Azure Cognitive Search when using it as an Azure OpenAI chat\n * extension.\n */\nexport interface AzureCognitiveSearchChatExtensionConfiguration {\n  /**\n   * The type label to use when configuring Azure OpenAI chat extensions. This should typically not be changed from its\n   * default value for Azure Cognitive Search.\n   */\n  type: \"AzureCognitiveSearch\";\n  /**\n   * The authentication method to use when accessing the defined data source.\n   * Each data source type supports a specific set of available authentication methods; please see the documentation of\n   * the data source for supported mechanisms.\n   * If not otherwise provided, On Your Data will attempt to use System Managed Identity (default credential)\n   * authentication.\n   */\n  authentication?: OnYourDataAuthenticationOptions;\n  /** The configured top number of documents to feature for the configured query. */\n  topNDocuments?: number;\n  /** Whether queries should be restricted to use of indexed data. */\n  inScope?: boolean;\n  /** The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. */\n  strictness?: number;\n  /** Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. */\n  roleInformation?: string;\n  /** The absolute endpoint path for the Azure Cognitive Search resource to use. */\n  endpoint: string;\n  /** The name of the index to use as available in the referenced Azure Cognitive Search resource. */\n  indexName: string;\n  /** The API key to use when interacting with the Azure Cognitive Search resource. */\n  key?: string;\n  /** Customized field mapping behavior to use when interacting with the search index. */\n  fieldsMapping?: AzureCognitiveSearchIndexFieldMappingOptions;\n  /** The query type to use with Azure Cognitive Search. */\n  queryType?: AzureCognitiveSearchQueryType;\n  /** The additional semantic configuration for the query. */\n  semanticConfiguration?: string;\n  /** Search filter. */\n  filter?: string;\n  /** When using embeddings for search, specifies the resource endpoint URL from which embeddings should be retrieved. It should be in the format of format `https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/embeddings?api-version={api-version}`. */\n  embeddingEndpoint?: string;\n  /** When using embeddings, specifies the API key to use with the provided embeddings endpoint. */\n  embeddingKey?: string;\n  /** The embedding dependency for vector search. */\n  embeddingDependency?: OnYourDataVectorizationSource;\n}\n\n/**\n * A specific representation of configurable options for Azure Machine Learning vector index when using it as an Azure\n * OpenAI chat extension.\n */\nexport interface AzureMachineLearningIndexChatExtensionConfiguration {\n  /**\n   * The type label to use when configuring Azure OpenAI chat extensions. This should typically not be changed from its\n   * default value for Azure Machine Learning vector index.\n   */\n  type: \"AzureMLIndex\";\n  /**\n   * The authentication method to use when accessing the defined data source.\n   * Each data source type supports a specific set of available authentication methods; please see the documentation of\n   * the data source for supported mechanisms.\n   * If not otherwise provided, On Your Data will attempt to use System Managed Identity (default credential)\n   * authentication.\n   */\n  authentication?: OnYourDataAuthenticationOptions;\n  /** The configured top number of documents to feature for the configured query. */\n  topNDocuments?: number;\n  /** Whether queries should be restricted to use of indexed data. */\n  inScope?: boolean;\n  /** The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. */\n  strictness?: number;\n  /** Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. */\n  roleInformation?: string;\n  /** The resource ID of the Azure Machine Learning project. */\n  projectResourceId: string;\n  /** The Azure Machine Learning vector index name. */\n  name: string;\n  /** The version of the Azure Machine Learning vector index. */\n  version: string;\n  /** Search filter. Only supported if the Azure Machine Learning vector index is of type AzureSearch. */\n  filter?: string;\n}\n\n/**\n * A specific representation of configurable options for Elasticsearch when using it as an Azure OpenAI chat\n * extension.\n */\nexport interface AzureCosmosDBChatExtensionConfiguration {\n  /**\n   * The type label to use when configuring Azure OpenAI chat extensions. This should typically not be changed from its\n   * default value for Azure Cosmos DB.\n   */\n  type: \"AzureCosmosDB\";\n  /**\n   * The authentication method to use when accessing the defined data source.\n   * Each data source type supports a specific set of available authentication methods; please see the documentation of\n   * the data source for supported mechanisms.\n   * If not otherwise provided, On Your Data will attempt to use System Managed Identity (default credential)\n   * authentication.\n   */\n  authentication?: OnYourDataAuthenticationOptions;\n  /** The configured top number of documents to feature for the configured query. */\n  topNDocuments?: number;\n  /** Whether queries should be restricted to use of indexed data. */\n  inScope?: boolean;\n  /** The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. */\n  strictness?: number;\n  /** Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. */\n  roleInformation?: string;\n  /** The MongoDB vCore database name to use with Azure Cosmos DB. */\n  databaseName: string;\n  /** The name of the Azure Cosmos DB resource container. */\n  containerName: string;\n  /** The MongoDB vCore index name to use with Azure Cosmos DB. */\n  indexName: string;\n  /** Customized field mapping behavior to use when interacting with the search index. */\n  fieldsMapping: AzureCosmosDBFieldMappingOptions;\n  /** The embedding dependency for vector search. */\n  embeddingDependency?: OnYourDataVectorizationSource;\n}\n\n/**\n * A specific representation of configurable options for Elasticsearch when using it as an Azure OpenAI chat\n * extension.\n */\nexport interface ElasticsearchChatExtensionConfiguration {\n  /**\n   * The type label to use when configuring Azure OpenAI chat extensions. This should typically not be changed from its\n   * default value for Elasticsearch®.\n   */\n  type: \"Elasticsearch\";\n  /**\n   * The authentication method to use when accessing the defined data source.\n   * Each data source type supports a specific set of available authentication methods; please see the documentation of\n   * the data source for supported mechanisms.\n   * If not otherwise provided, On Your Data will attempt to use System Managed Identity (default credential)\n   * authentication.\n   */\n  authentication?: OnYourDataAuthenticationOptions;\n  /** The configured top number of documents to feature for the configured query. */\n  topNDocuments?: number;\n  /** Whether queries should be restricted to use of indexed data. */\n  inScope?: boolean;\n  /** The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. */\n  strictness?: number;\n  /** Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. */\n  roleInformation?: string;\n  /** The endpoint of Elasticsearch®. */\n  endpoint: string;\n  /** The index name of Elasticsearch®. */\n  indexName: string;\n  /** The index field mapping options of Elasticsearch®. */\n  fieldsMapping?: ElasticsearchIndexFieldMappingOptions;\n  /** The query type of Elasticsearch®. */\n  queryType?: ElasticsearchQueryType;\n  /** The embedding dependency for vector search. */\n  embeddingDependency?: OnYourDataVectorizationSource;\n}\n\n/**\n * A specific representation of configurable options for Elasticsearch when using it as an Azure OpenAI chat\n * extension.\n */\nexport interface PineconeChatExtensionConfiguration {\n  /**\n   * The type label to use when configuring Azure OpenAI chat extensions. This should typically not be changed from its\n   * default value for Pinecone.\n   */\n  type: \"Pinecone\";\n  /**\n   * The authentication method to use when accessing the defined data source.\n   * Each data source type supports a specific set of available authentication methods; please see the documentation of\n   * the data source for supported mechanisms.\n   * If not otherwise provided, On Your Data will attempt to use System Managed Identity (default credential)\n   * authentication.\n   */\n  authentication?: OnYourDataAuthenticationOptions;\n  /** The configured top number of documents to feature for the configured query. */\n  topNDocuments?: number;\n  /** Whether queries should be restricted to use of indexed data. */\n  inScope?: boolean;\n  /** The configured strictness of the search relevance filtering. The higher of strictness, the higher of the precision but lower recall of the answer. */\n  strictness?: number;\n  /** Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality and tell it how to format responses. There's a 100 token limit for it, and it counts against the overall token limit. */\n  roleInformation?: string;\n  /** The environment name of Pinecone. */\n  environment: string;\n  /** The name of the Pinecone database index. */\n  indexName: string;\n  /** Customized field mapping behavior to use when interacting with the search index. */\n  fieldsMapping: PineconeFieldMappingOptions;\n  /** The embedding dependency for vector search. */\n  embeddingDependency?: OnYourDataVectorizationSource;\n}\n\n/**\n *   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat\n *   completions request that should use Azure OpenAI chat extensions to augment the response behavior.\n *   The use of this configuration is compatible only with Azure OpenAI.\n */\nexport type AzureChatExtensionConfiguration =\n  | AzureCognitiveSearchChatExtensionConfiguration\n  | AzureMachineLearningIndexChatExtensionConfiguration\n  | AzureCosmosDBChatExtensionConfiguration\n  | ElasticsearchChatExtensionConfiguration\n  | PineconeChatExtensionConfiguration;\n\n/** The authentication options for Azure OpenAI On Your Data when using an API key. */\nexport interface OnYourDataApiKeyAuthenticationOptions {\n  /** The authentication type of API key. */\n  type: \"APIKey\";\n  /** The API key to use for authentication. */\n  key: string;\n}\n\n/** The authentication options for Azure OpenAI On Your Data when using a connection string. */\nexport interface OnYourDataConnectionStringAuthenticationOptions {\n  /** The authentication type of connection string. */\n  type: \"ConnectionString\";\n  /** The connection string to use for authentication. */\n  connectionString: string;\n}\n\n/** The authentication options for Azure OpenAI On Your Data when using an Elasticsearch key and key ID pair. */\nexport interface OnYourDataKeyAndKeyIdAuthenticationOptions {\n  /** The authentication type of Elasticsearch key and key ID pair. */\n  type: \"KeyAndKeyId\";\n  /** The key to use for authentication. */\n  key: string;\n  /** The key ID to use for authentication. */\n  keyId: string;\n}\n\n/** The authentication options for Azure OpenAI On Your Data when using a system-assigned managed identity. */\nexport interface OnYourDataSystemAssignedManagedIdentityAuthenticationOptions {\n  /** The authentication type of system-assigned managed identity. */\n  type: \"SystemAssignedManagedIdentity\";\n}\n\n/** The authentication options for Azure OpenAI On Your Data when using a user-assigned managed identity. */\nexport interface OnYourDataUserAssignedManagedIdentityAuthenticationOptions {\n  /** The authentication type of user-assigned managed identity. */\n  type: \"UserAssignedManagedIdentity\";\n  /** The resource ID of the user-assigned managed identity to use for authentication. */\n  managedIdentityResourceId: string;\n}\n\n/** The authentication options for Azure OpenAI On Your Data. */\nexport type OnYourDataAuthenticationOptions =\n  | OnYourDataApiKeyAuthenticationOptions\n  | OnYourDataConnectionStringAuthenticationOptions\n  | OnYourDataKeyAndKeyIdAuthenticationOptions\n  | OnYourDataSystemAssignedManagedIdentityAuthenticationOptions\n  | OnYourDataUserAssignedManagedIdentityAuthenticationOptions;\n\n/**\n * The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\n * on a public Azure OpenAI endpoint call for embeddings.\n */\nexport interface OnYourDataEndpointVectorizationSource {\n  /** The type of vectorization source to use. Always 'Endpoint' for this type. */\n  type: \"Endpoint\";\n  /** Specifies the resource endpoint URL from which embeddings should be retrieved. It should be in the format of https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/embeddings. The api-version query parameter is not allowed. */\n  endpoint: string;\n  /** Specifies the authentication options to use when retrieving embeddings from the specified endpoint. */\n  authentication: OnYourDataAuthenticationOptions;\n}\n\n/**\n * The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\n * on an internal embeddings model deployment name in the same Azure OpenAI resource.\n */\nexport interface OnYourDataDeploymentNameVectorizationSource {\n  /** The type of vectorization source to use. Always 'DeploymentName' for this type. */\n  type: \"DeploymentName\";\n  /** The embedding model deployment name within the same Azure OpenAI resource. This enables you to use vector search without Azure OpenAI api-key and without Azure OpenAI public network access. */\n  deploymentName: string;\n}\n\n/**\n * The details of a a vectorization source, used by Azure OpenAI On Your Data when applying vector search, that is based\n * on a search service model ID. Currently only supported by Elasticsearch®.\n */\nexport interface OnYourDataModelIdVectorizationSource {\n  /** The type of vectorization source to use. Always 'ModelId' for this type. */\n  type: \"ModelId\";\n  /** The embedding model ID build inside the search service. Currently only supported by Elasticsearch®. */\n  modelId: string;\n}\n\n/** A representation of a vectorization source for Azure OpenAI On Your Data with vector search. */\nexport type OnYourDataVectorizationSource =\n  | OnYourDataEndpointVectorizationSource\n  | OnYourDataDeploymentNameVectorizationSource\n  | OnYourDataModelIdVectorizationSource;\n\n/** A readable stream that is iterable and disposable. */\nexport interface EventStream<T> extends ReadableStream<T>, AsyncIterable<T> {}\n"]}