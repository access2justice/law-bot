{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobalt import AkomaNtosoDocument\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['822.113.xml', '822.112.xml', '822.111.xml', '822.115.xml', '822.114.xml']\n"
     ]
    }
   ],
   "source": [
    "file_folder_xml = 'files_xml'\n",
    "xml_files = os.listdir(file_folder_xml)\n",
    "print(xml_files)\n",
    "\n",
    "\n",
    "file_folder_json = 'files_json'\n",
    "if not os.path.exists(file_folder_json): \n",
    "    os.mkdir(file_folder_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element_clean_text(el):\n",
    "    if el is None:\n",
    "        return ''\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    if el.text:\n",
    "        texts.append(el.text.replace('\\xa0', ' ').strip())\n",
    "        print('el text: ',el.text)\n",
    "\n",
    "    for child_el in el.iterchildren():\n",
    "        if child_el is not None and child_el.tag not in excluded_text_tags:\n",
    "            child_txt = texts.append(get_element_clean_text(child_el))\n",
    "            print('child_el text: ', child_el.text)\n",
    "            if child_txt:\n",
    "                texts.append(child_txt)\n",
    "        if child_el.tail:\n",
    "            texts.append(child_el.tail.replace('\\xa0', ' ').strip())\n",
    "    texts_joined = ' '.join(texts)\n",
    "    print('texts_joined: ', texts_joined)\n",
    "    return texts_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraph_blocklist(lst, blocklist, article_lnk, section_titles, article_title,  level_eid):\n",
    "    if blocklist is None:\n",
    "        return\n",
    "\n",
    "    # Within Blocklist, some articles have an ListIntroduction others don't. The Function handles both cases.\n",
    "    # Process list introduction, if found\n",
    "    if hasattr(blocklist, 'listIntroduction'):\n",
    "        list_intro = blocklist.listIntroduction\n",
    "        level_eid = list_intro.attrib[\"eId\"]\n",
    "        paragraph_txt = get_element_clean_text(list_intro)\n",
    "        if paragraph_txt:\n",
    "            lst.append({'text': paragraph_txt, 'metadata': article_lnk + doc_title + section_titles + article_title, '@eId': level_eid})\n",
    "\n",
    "    # Process any list items (this allows to capture enumerated paragraphs), ex.: Art. 958 C\n",
    "    if hasattr(blocklist, 'item'):\n",
    "        for item in blocklist.item:\n",
    "            if hasattr(item, 'p'):\n",
    "                level_eid = item.attrib[\"eId\"]\n",
    "                paragraph_txt = get_element_clean_text(item.p)\n",
    "                if paragraph_txt:\n",
    "                    lst.append({'text': paragraph_txt, 'metadata': article_lnk + doc_title + section_titles + article_title, '@eId': level_eid})\n",
    "\n",
    "            # Handle blocklists nested within items\n",
    "            if hasattr(item, 'blockList'):\n",
    "                process_paragraph_blocklist(lst, item.blockList, article_lnk, section_titles, article_title, level_eid)\n",
    "\n",
    "\n",
    "def process_article(lst, article, section_titles, level_eid):\n",
    "    \"\"\"\n",
    "    Extract paragraphs from the different sections.\n",
    "    Add paragraphs, metadata, and Id to a dictionary with the format:\n",
    "    {\"text\": \"string containing each paragraph individually\",\n",
    "    \"metadata\": [Article Link + Article number + sections names in one list](list),\n",
    "    \"@eId\": article and paragraph information (inner \"eId\" attribute)}\n",
    "    \"\"\"\n",
    "    # Check article\n",
    "    if article is None:\n",
    "        return lst\n",
    "\n",
    "    article_title = [str(' '.join([str(el.text).replace('\\xa0', ' ').strip() for el in article.num.getchildren()]).strip())]\n",
    "    article_eid = article.attrib[\"eId\"]\n",
    "    article_url = [f'{base_url}#{article_eid}']\n",
    "\n",
    "    # this excludes articles with no paragraphs like Art. 40g\n",
    "    if not hasattr(article, 'paragraph') or len(article.paragraph) == 0:\n",
    "        return lst\n",
    "\n",
    "    for paragraph in article.paragraph:\n",
    "        # Extract Paragraphs from Articles. Will cover the articles where these are not nested like Art. 1\n",
    "        if hasattr(paragraph.content, 'p'):\n",
    "            # print('paragraph.content: ', paragraph.content.p)\n",
    "            # for p in paragraph.content:\n",
    "            #     print('paragraph child:', p)\n",
    "            level_eid = paragraph.attrib['eId']\n",
    "            paragraph_txt = get_element_clean_text(paragraph.content)\n",
    "\n",
    "\n",
    "            if paragraph_txt:\n",
    "                lst.append({'text': paragraph_txt, 'metadata': article_url + doc_title +  section_titles + article_title, '@eId': level_eid})\n",
    "\n",
    "        # When an article has blocklist within content, it will call the function process_paragraph_blocklist\n",
    "        # This occurs where there are enumerated items within an article. Ex. Art. 24\n",
    "        if hasattr(paragraph.content, 'blockList'):\n",
    "            process_paragraph_blocklist(lst, paragraph.content.blockList, article_url, section_titles, article_title, level_eid)\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "def process_sections(lst, sections, section_titles, level_eid='N/A'):\n",
    "    \"\"\"Retrieve all the sections and call function find_article\"\"\"\n",
    "    for section in sections:\n",
    "        if hasattr(section, 'num') or not section.num:\n",
    "\n",
    "            section_title = str(get_element_clean_text(section.num)).replace('\\xa0', ' ').strip()\n",
    "            if hasattr(section, 'heading'):\n",
    "                section_title += ' ' + str(' '.join([headingtext.text.strip() for headingtext in section.heading])).replace('\\xa0', ' ').strip()\n",
    "            lst = find_articles(lst, section, section_titles + [section_title], level_eid)\n",
    "        else:\n",
    "            lst = find_articles(lst, section, section_titles, level_eid)\n",
    "\n",
    "    return lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_articles(lst, parent, section_titles, level_eid='N/A'):\n",
    "    \"\"\"Look for article tag to trigger the function process_article\"\"\"\n",
    "    # Check parent\n",
    "    if parent is None:\n",
    "        return lst\n",
    "\n",
    "    # Find all articles that are children of this parent\n",
    "    if hasattr(parent, 'article'):\n",
    "        for article in parent.article:\n",
    "            lst = process_article(lst, article, section_titles, level_eid)\n",
    "\n",
    "    # Find all sections of type part\n",
    "    if hasattr(parent, 'part'):\n",
    "        # print('FOUND PARTS')\n",
    "        lst = process_sections(lst, parent.part, section_titles, level_eid)\n",
    "\n",
    "    if hasattr(parent, 'section'):\n",
    "        lst = process_sections(lst, parent.section, section_titles, level_eid)\n",
    "\n",
    "    # Find all sections of type title\n",
    "    if hasattr(parent, 'title'):\n",
    "        lst = process_sections(lst, parent.title, section_titles, level_eid)\n",
    "\n",
    "    # Find all sections of type chapter\n",
    "    if hasattr(parent, 'chapter'):\n",
    "        lst = process_sections(lst, parent.chapter, section_titles, level_eid)\n",
    "\n",
    "    # Find all sections of type level\n",
    "    if hasattr(parent, 'level'):\n",
    "        level_eid = parent.level.attrib['eId']\n",
    "        lst = process_sections(lst, parent.level, section_titles, level_eid)\n",
    "\n",
    "    # Find all sections of type book\n",
    "    if hasattr(parent, 'book'):\n",
    "        lst = process_sections(lst, parent.book, section_titles)\n",
    "\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822.111.xml\n",
      "<cobalt.akn.AkomaNtosoDocument object at 0x10b8621e0>\n",
      "https://fedlex.admin.ch/eli/cc/2000/243/de\n",
      "el text:  Verordnung 1\n",
      "texts_joined:  \n",
      "child_el text:  None\n",
      "texts_joined:  Verordnung 1  zum Arbeitsgesetz\n",
      "el text:  (ArGV 1)\n",
      "texts_joined:  (ArGV 1)\n",
      "False\n",
      "el text:  1. Kapitel: Geltungsbereich\n",
      "texts_joined:  1. Kapitel: Geltungsbereich\n",
      "el text:  1. Abschnitt: Begriffe\n",
      "texts_joined:  1. Abschnitt: Begriffe\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     25\u001b[0m list_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 26\u001b[0m list_data \u001b[38;5;241m=\u001b[39m \u001b[43mfind_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(list_data)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# save it to json file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[199], line 26\u001b[0m, in \u001b[0;36mfind_articles\u001b[0;34m(lst, parent, section_titles, level_eid)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Find all sections of type chapter\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Find all sections of type level\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[190], line 77\u001b[0m, in \u001b[0;36mprocess_sections\u001b[0;34m(lst, sections, section_titles, level_eid)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(section, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     76\u001b[0m         section_title \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([headingtext\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m headingtext \u001b[38;5;129;01min\u001b[39;00m section\u001b[38;5;241m.\u001b[39mheading]))\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 77\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43mfind_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_titles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msection_title\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     lst \u001b[38;5;241m=\u001b[39m find_articles(lst, section, section_titles, level_eid)\n",
      "Cell \u001b[0;32mIn[199], line 18\u001b[0m, in \u001b[0;36mfind_articles\u001b[0;34m(lst, parent, section_titles, level_eid)\u001b[0m\n\u001b[1;32m     15\u001b[0m     lst \u001b[38;5;241m=\u001b[39m process_sections(lst, parent\u001b[38;5;241m.\u001b[39mpart, section_titles, level_eid)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Find all sections of type title\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[190], line 77\u001b[0m, in \u001b[0;36mprocess_sections\u001b[0;34m(lst, sections, section_titles, level_eid)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(section, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     76\u001b[0m         section_title \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([headingtext\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m headingtext \u001b[38;5;129;01min\u001b[39;00m section\u001b[38;5;241m.\u001b[39mheading]))\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 77\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43mfind_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_titles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msection_title\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     lst \u001b[38;5;241m=\u001b[39m find_articles(lst, section, section_titles, level_eid)\n",
      "Cell \u001b[0;32mIn[199], line 10\u001b[0m, in \u001b[0;36mfind_articles\u001b[0;34m(lst, parent, section_titles, level_eid)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m parent\u001b[38;5;241m.\u001b[39marticle:\n\u001b[0;32m---> 10\u001b[0m         lst \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msection_titles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_eid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find all sections of type part\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print('FOUND PARTS')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[190], line 54\u001b[0m, in \u001b[0;36mprocess_article\u001b[0;34m(lst, article, section_titles, level_eid)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m article\u001b[38;5;241m.\u001b[39mparagraph:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Extract Paragraphs from Articles. Will cover the articles where these are not nested like Art. 1\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(paragraph\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# print('paragraph.content: ', paragraph.content.p)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# for p in paragraph.content:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m#     print('paragraph child:', p)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         level_eid \u001b[38;5;241m=\u001b[39m \u001b[43mparagraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrib\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m         paragraph_txt \u001b[38;5;241m=\u001b[39m get_element_clean_text(paragraph\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m paragraph_txt:\n",
      "File \u001b[0;32msrc/lxml/etree.pyx:2504\u001b[0m, in \u001b[0;36mlxml.etree._Attrib.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eId'"
     ]
    }
   ],
   "source": [
    "\n",
    "xml_files=['822.111.xml']\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    # open xml file\n",
    "    print(xml_file)\n",
    "    with open(os.path.join(file_folder_xml, xml_file), \"rb\") as file:\n",
    "        xml_data = file.read()\n",
    "    \n",
    "    akn_doc = AkomaNtosoDocument(xml_data)\n",
    "    print(akn_doc)\n",
    "    base_url = akn_doc.root.act.meta.identification.FRBRExpression.FRBRuri.attrib[\"value\"]\n",
    "    base_url = base_url.replace('fedlex.data.admin', 'fedlex.admin')\n",
    "    base_url = re.sub(r'\\/\\d{8}(\\/\\w+)$', '\\\\1', base_url)\n",
    "    print(base_url)\n",
    "\n",
    "    excluded_text_tags = [f'{{{akn_doc.namespace}}}{tag}' for tag in ['authorialNote', 'ref', 'num']]\n",
    "\n",
    "    #get title and num of the document\n",
    "    doc_title = [get_element_clean_text(akn_doc.root.act.preface.p[1].docTitle) + ' ' + get_element_clean_text(akn_doc.root.act.preface.p[2])]\n",
    "    doc_num = ['SR ' + akn_doc.root.act.preface.p[0].docNumber.text]\n",
    "\n",
    "    body = akn_doc.root.act.body\n",
    "    print(hasattr(body, 'section'))\n",
    "\n",
    "    list_data = []\n",
    "    list_data = find_articles(list_data, body, [])\n",
    "    print(list_data)\n",
    "\n",
    "    # save it to json file\n",
    "    file_path_json = os.path.join(file_folder_json, xml_file.replace('.xml', '.json'))\n",
    "\n",
    "    with open(file_path_json, 'w', encoding='utf-8') as file:\n",
    "        json.dump(list_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    by_article = {}\n",
    "    for elem in list_data:\n",
    "        level_key = elem['metadata'][-1]\n",
    "        article_key = elem['metadata'][3]\n",
    "        new_key = level_key + article_key\n",
    "        elem['metadata'].pop()\n",
    "        # elem['metadata'][-1] += ' ZGB'\n",
    "        if new_key not in by_article:\n",
    "            by_article[new_key] = {'text': elem['text'], 'metadata': elem['metadata'], '@eIds': [elem['@eId']]}\n",
    "        else:\n",
    "            by_article[new_key]['text'] += ' ' + elem['text']\n",
    "            by_article[new_key]['@eIds'].append(elem['@eId'])\n",
    "\n",
    "    # removing the article link as a key that was used to group the data so that he exported data in the json format has the same structure\n",
    "    values_by_article = list(by_article.values())\n",
    "\n",
    "    file_path_json_by_article = os.path.join(file_folder_json, xml_file.replace('.xml', '_by_article.json'))\n",
    "    with open(file_path_json_by_article, 'w', encoding='utf-8') as file:\n",
    "        json.dump(values_by_article, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedlex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
